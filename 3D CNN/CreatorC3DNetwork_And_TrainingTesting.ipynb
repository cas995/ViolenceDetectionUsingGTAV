{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "Using Keras 2.2.4\n",
      "TensorFlow version: 1.15.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(\"Using Keras\",keras.__version__)\n",
    "import tensorflow as tf\n",
    "print (\"TensorFlow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 3D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, ZeroPadding3D, Dropout, Input, Activation,BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_model2(summary=False):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        Return the model of the network\n",
    "    \"\"\"  \n",
    "    model = Sequential()\n",
    "\n",
    "    #1st layer of model\n",
    "    model.add(Conv3D(64, kernel_size=(3, 3, 3), #kernel_regularizer=keras.regularizers.l1_l2(0.001), #bias_regularizer=l2(0.01),#kernel_initializer='he_uniform',activation='relu',\n",
    "                    padding='same', name='conv1', \n",
    "                    strides=(1, 1, 1), input_shape=(16, 112, 112, 3)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), \n",
    "                            padding='valid', name='pool1'))\n",
    "    #2nd layer of model\n",
    "    model.add(Conv3D(128, kernel_size=(3, 3, 3), #kernel_regularizer=keras.regularizers.l1_l2(0.001), #kernel_regularizer=l2(0.001), #bias_regularizer=l2(0.01),#kernel_initializer='he_uniform',activation='relu',\n",
    "                    padding='same', name='conv2',\n",
    "                    strides=(1, 1, 1)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), \n",
    "                            padding='valid', name='pool2'))\n",
    "    #3rd layer of model\n",
    "    model.add(Conv3D(256, kernel_size=(3, 3, 3), #kernel_regularizer=keras.regularizers.l1_l2(0.001), #kernel_regularizer=l2(0.001), #bias_regularizer=l2(0.01),#kernel_initializer='he_uniform',activation='relu',\n",
    "                    padding='same', name='conv3a',\n",
    "                    strides=(1, 1, 1)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv3D(256, kernel_size=(3, 3, 3), #kernel_regularizer=keras.regularizers.l1_l2(0.001), #kernel_regularizer=l2(0.001),  #bias_regularizer=l2(0.01),#kernel_initializer='he_uniform',activation='relu',\n",
    "                    padding='same', name='conv3b',\n",
    "                    strides=(1, 1, 1)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), \n",
    "                            padding='valid', name='pool3'))\n",
    "    #4th layer of model\n",
    "    model.add(Conv3D(512, kernel_size=(3, 3, 3), #kernel_regularizer=keras.regularizers.l1_l2(0.001), #kernel_regularizer=l2(0.001), #bias_regularizer=l2(0.01),#kernel_initializer='he_uniform',activation='relu',\n",
    "                    padding='same', name='conv4a',\n",
    "                      strides=(1, 1, 1)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv3D(512, kernel_size=(3, 3, 3), #kernel_regularizer=keras.regularizers.l1_l2(0.001), #kernel_regularizer=l2(0.001), #bias_regularizer=l2(0.01),#kernel_initializer='he_uniform',activation='relu',\n",
    "                    padding='same', name='conv4b',\n",
    "                    strides=(1, 1, 1)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), \n",
    "                            padding='valid', name='pool4'))\n",
    "    #5th layer of model\n",
    "    model.add(Conv3D(512, kernel_size=(3, 3, 3), #kernel_regularizer=keras.regularizers.l1_l2(0.001), #kernel_regularizer=l2(0.001), #bias_regularizer=l2(0.01),#kernel_initializer='he_uniform',activation='relu',\n",
    "                    padding='same', name='conv5a',\n",
    "                    strides=(1, 1, 1)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv3D(512, kernel_size=(3, 3, 3),# kernel_regularizer=keras.regularizers.l1_l2(0.001), #kernel_regularizer=l2(0.001), #bias_regularizer=l2(0.01),#kernel_initializer='he_uniform',activation='relu',\n",
    "                    padding='same', name='conv5b',\n",
    "                    strides=(1, 1, 1)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(ZeroPadding3D(padding=(0, 1, 1)))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), \n",
    "                            padding='valid', name='pool5'))\n",
    "    model.add(Flatten())\n",
    "    # FC layers group\n",
    "    model.add(Dense(4096, activation='relu', name='fc6'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(4096, activation='relu', name='fc7'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid', name='fc8'))\n",
    "    if summary:\n",
    "        print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the 3D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 16, 112, 112, 64)  5248      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16, 112, 112, 64)  256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16, 112, 112, 64)  0         \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 16, 56, 56, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 16, 56, 56, 128)   221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 56, 56, 128)   512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 56, 56, 128)   0         \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 8, 28, 28, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3a (Conv3D)              (None, 8, 28, 28, 256)    884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 28, 28, 256)    1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 28, 28, 256)    0         \n",
      "_________________________________________________________________\n",
      "conv3b (Conv3D)              (None, 8, 28, 28, 256)    1769728   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 28, 28, 256)    1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 28, 28, 256)    0         \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 4, 14, 14, 256)    0         \n",
      "_________________________________________________________________\n",
      "conv4a (Conv3D)              (None, 4, 14, 14, 512)    3539456   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 14, 14, 512)    2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 14, 14, 512)    0         \n",
      "_________________________________________________________________\n",
      "conv4b (Conv3D)              (None, 4, 14, 14, 512)    7078400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 14, 14, 512)    2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 14, 14, 512)    0         \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling3D)         (None, 2, 7, 7, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv5a (Conv3D)              (None, 2, 7, 7, 512)      7078400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 7, 7, 512)      2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2, 7, 7, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv5b (Conv3D)              (None, 2, 7, 7, 512)      7078400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 2, 7, 7, 512)      2048      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2, 7, 7, 512)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding3d (ZeroPadding3 (None, 2, 9, 9, 512)      0         \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling3D)         (None, 1, 4, 4, 512)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 78,010,881\n",
      "Trainable params: 78,005,377\n",
      "Non-trainable params: 5,504\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7ffab561c780>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = get_model2(summary=True)   \n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X-data: Combine augmentation techniques with original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "client = boto3.client('s3')\n",
    "bucket = \"thesis-data-augmentation-data\"\n",
    "\n",
    "file_Xdata_MixTraditionalTechniquesAugmentationTrain = \"X_data_MixTraditionalTechniquesAugmentationTrain.npy\"\n",
    "obj_x_train_style = client.get_object(Bucket= bucket, Key=file_Xdata_MixTraditionalTechniquesAugmentationTrain)\n",
    "array_x_train = np.load(BytesIO(obj_x_train_style['Body'].read()))\n",
    "\n",
    "print('successfully loaded augmentation train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Xdata_StyleAugmentationTrain = \"X_data_StyleAugmentationTrain.npy\"\n",
    "obj_x_train_styleAugmentation = client.get_object(Bucket= bucket, Key=file_Xdata_StyleAugmentationTrain)\n",
    "arrayStyleAug_x_train = np.load(BytesIO(obj_x_train_styleAugmentation['Body'].read()))\n",
    "\n",
    "print('successfully loaded style augmentation train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(arrayStyleAug_x_train))\n",
    "array_x_train = array_x_train[:468]\n",
    "arrayStyleAug_x_train = arrayStyleAug_x_train[468:935] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xdata_concatenated_array_train = np.concatenate((array_x_train,arrayStyleAug_x_train))\n",
    "print('Xtrain concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Xdata_OriginalAugmentationTrain = \"X_data_OriginalImagesTrain.npy\"\n",
    "obj_x_train_original = client.get_object(Bucket= bucket, Key=file_Xdata_OriginalAugmentationTrain)\n",
    "array_x_train_original = np.load(BytesIO(obj_x_train_original['Body'].read()))\n",
    "\n",
    "print('successfully loaded original train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xdata_concatenated_array_train = np.concatenate((array_x_train_original,xdata_concatenated_array_train))\n",
    "print('XtrainMix concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Xdata_OriginalAugmentationTrain =[]\n",
    "obj_x_train_original =[]\n",
    "array_x_train_original =[]\n",
    "arrayStyleAug_x_train =[]\n",
    "array_x_train =[]\n",
    "file_Xdata_StyleAugmentationTrain =[]\n",
    "file_Xdata_MixTraditionalTechniquesAugmentationTrain =[]\n",
    "obj_x_train_style = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "client = boto3.client('s3')\n",
    "bucket = \"thesis-data-augmentation-data\"\n",
    "\n",
    "file_Xdata_MixTraditionalTechniquesAugmentationTest = \"X_data_MixTraditionalTechniquesAugmentationTest.npy\"\n",
    "obj_x_test_style = client.get_object(Bucket= bucket, Key=file_Xdata_MixTraditionalTechniquesAugmentationTest)\n",
    "array_x_test = np.load(BytesIO(obj_x_test_style['Body'].read()))\n",
    "\n",
    "print('successfully loaded augmentation test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Xdata_StyleAugmentationTest = \"X_data_StyleAugmentationTest.npy\"\n",
    "obj_x_test_styleAugmentation = client.get_object(Bucket= bucket, Key=file_Xdata_StyleAugmentationTest)\n",
    "arrayStyleAug_x_test = np.load(BytesIO(obj_x_test_styleAugmentation['Body'].read()))\n",
    "\n",
    "print('successfully loaded style augmentation test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(array_x_test))\n",
    "array_x_test = array_x_test[:156]\n",
    "arrayStyleAug_x_test = arrayStyleAug_x_test[156:312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xdata_concatenated_array_test = np.concatenate((array_x_test,arrayStyleAug_x_test))\n",
    "print('XtestMix concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Xdata_OriginalAugmentationTest = \"X_data_OriginalImagesTest.npy\"\n",
    "obj_x_test_original = client.get_object(Bucket= bucket, Key=file_Xdata_OriginalAugmentationTest)\n",
    "array_x_test_original = np.load(BytesIO(obj_x_test_original['Body'].read()))\n",
    "\n",
    "print('successfully loaded original test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xdata_concatenated_array_test = np.concatenate((array_x_test_original,xdata_concatenated_array_test))\n",
    "print('Xtest concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Xdata_OriginalAugmentationTest =[]\n",
    "obj_x_test_original =[]\n",
    "array_x_test_original =[]\n",
    "arrayStyleAug_x_test =[]\n",
    "array_x_test =[]\n",
    "file_Xdata_StyleAugmentationTest =[]\n",
    "file_Xdata_MixTraditionalTechniquesAugmentationTest =[]\n",
    "obj_x_test_style = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y-data: Combine augmentation techniques with original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "client = boto3.client('s3')\n",
    "bucket = \"thesis-data-augmentation-data\"\n",
    "\n",
    "file_Ydata_MixTraditionalTechniquesAugmentationTrain = \"Y_data_MixTraditionalTechniquesAugmentationTrain.npy\"\n",
    "obj_y_train_style = client.get_object(Bucket= bucket, Key=file_Ydata_MixTraditionalTechniquesAugmentationTrain)\n",
    "array_y_train = np.load(BytesIO(obj_y_train_style['Body'].read()))\n",
    "\n",
    "print('successfully loaded augmentation train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Ydata_StyleAugmentationTrain = \"Y_data_StyleAugmentationTrain.npy\"\n",
    "obj_y_train_styleAugmentation = client.get_object(Bucket= bucket, Key=file_Ydata_StyleAugmentationTrain)\n",
    "arrayStyleAug_y_train = np.load(BytesIO(obj_y_train_styleAugmentation['Body'].read()))\n",
    "\n",
    "print('successfully loaded style augmentation train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(arrayStyleAug_y_train))\n",
    "array_y_train = array_y_train[:468]\n",
    "arrayStyleAug_y_train= arrayStyleAug_y_train[468:935]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ydata_concatenated_array_train = np.concatenate((array_y_train,arrayStyleAug_y_train))\n",
    "print('Ytrain concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Ydata_OriginalAugmentationTrain = \"Y_data_OriginalImagesTrain.npy\"\n",
    "obj_y_train_original = client.get_object(Bucket= bucket, Key=file_Ydata_OriginalAugmentationTrain)\n",
    "array_y_train_original = np.load(BytesIO(obj_y_train_original['Body'].read()))\n",
    "\n",
    "print('successfully loaded original train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ydata_concatenated_array_train = np.concatenate((array_y_train_original,ydata_concatenated_array_train))\n",
    "print('YtrainMix concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Ydata_OriginalAugmentationTrain =[]\n",
    "obj_y_train_original =[]\n",
    "array_y_train_original =[]\n",
    "arrayStyleAug_y_train =[]\n",
    "array_y_train =[]\n",
    "file_Ydata_StyleAugmentationTrain =[]\n",
    "file_Ydata_MixTraditionalTechniquesAugmentationTrain =[]\n",
    "obj_y_train_style = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "client = boto3.client('s3')\n",
    "bucket = \"thesis-data-augmentation-data\"\n",
    "\n",
    "file_Ydata_MixTraditionalTechniquesAugmentationTest = \"Y_data_MixTraditionalTechniquesAugmentationTest.npy\"\n",
    "obj_y_test_style = client.get_object(Bucket= bucket, Key=file_Ydata_MixTraditionalTechniquesAugmentationTest)\n",
    "array_y_test = np.load(BytesIO(obj_y_test_style['Body'].read()))\n",
    "\n",
    "print('successfully loaded augmentation test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Ydata_StyleAugmentationTest = \"Y_data_StyleAugmentationTest.npy\"\n",
    "obj_y_test_styleAugmentation = client.get_object(Bucket= bucket, Key=file_Ydata_StyleAugmentationTest)\n",
    "arrayStyleAug_y_test = np.load(BytesIO(obj_y_test_styleAugmentation['Body'].read()))\n",
    "\n",
    "print('successfully loaded style augmentation test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(arrayStyleAug_y_test))\n",
    "array_y_test = array_y_test[:156]\n",
    "arrayStyleAug_y_test= arrayStyleAug_y_test[156:312]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ydata_concatenated_array_test = np.concatenate((array_y_test,arrayStyleAug_y_test))\n",
    "print('Ytest concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Ydata_OriginalAugmentationTest = \"Y_data_OriginalImagesTest.npy\"\n",
    "obj_y_test_original = client.get_object(Bucket= bucket, Key=file_Ydata_OriginalAugmentationTest)\n",
    "array_y_test_original = np.load(BytesIO(obj_y_test_original['Body'].read()))\n",
    "\n",
    "print('successfully loaded original test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ydata_concatenated_array_test = np.concatenate((array_y_test_original,ydata_concatenated_array_test))\n",
    "print('YestMix concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Ydata_OriginalAugmentationTest =[]\n",
    "obj_y_test_original =[]\n",
    "array_y_test_original =[]\n",
    "arrayStyleAug_y_test =[]\n",
    "array_y_test =[]\n",
    "file_Ydata_StyleAugmentationTest =[]\n",
    "file_Ydata_MixTraditionalTechniquesAugmentationTest =[]\n",
    "obj_y_test_style = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load original data (without augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"thesis-data-augmentation-data\"\n",
    "file = \"leeftijd_top_10.xlsx\"\n",
    "file_Xdata_augmentationTryTrain = \"X_data_AugmentationTryTrain.npy\"\n",
    "file_Xdata_augmentationTryTest = \"X_data_AugmentationTryTest.npy\"\n",
    "file_Ydata_augmentationTryTrain = \"Y_data_AugmentationTryTrainOneList.npy\"\n",
    "file_Ydata_augmentationTryTest = \"Y_data_AugmentationTryTestOneList.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "obj_x_train = client.get_object(Bucket= bucket, Key=file_Xdata_augmentationTryTrain)\n",
    "obj_x_test = client.get_object(Bucket= bucket, Key=file_Xdata_augmentationTryTest)\n",
    "obj_y_train = client.get_object(Bucket= bucket, Key=file_Ydata_augmentationTryTrain)\n",
    "obj_y_test = client.get_object(Bucket= bucket, Key=file_Ydata_augmentationTryTest)\n",
    "\n",
    "array_x_train = np.load(BytesIO(obj_x_train['Body'].read()))\n",
    "array_x_test = np.load(BytesIO(obj_x_test['Body'].read()))\n",
    "array_y_train = np.load(BytesIO(obj_y_train['Body'].read()))\n",
    "array_y_test = np.load(BytesIO(obj_y_test['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get summary of loaded data\n",
    "def data_summary(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Summarize current state of dataset\"\"\"\n",
    "    print('Train images shape:', X_train.shape)\n",
    "    print('Train labels shape:', y_train.shape)\n",
    "    print('Test images shape:', X_test.shape)\n",
    "    print('Test labels shape:', y_test.shape)\n",
    "    print('Train labels:', y_train)\n",
    "    print('Test labels:', y_test)\n",
    "\n",
    "data_summary(array_x_train, array_y_train, array_x_test, array_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = array_x_test[-625:]\n",
    "y_val = array_y_test[-625:]\n",
    "array_x_test = array_x_test[:625]\n",
    "array_y_test = array_y_test[:625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    nr_epochs = 40\n",
    "    batches_size = 16\n",
    "\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "    sgd = SGD(lr=0.001)\n",
    "\n",
    "    model2.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Save the model\n",
    "    model_json = model2.to_json()\n",
    "    with open(\"basicweights/model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # Save the weights using a checkpoint and save it model parameters to file each time it get better results.\n",
    "    filepath=\"basicweights/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    \n",
    "    full_history = []\n",
    "\n",
    "\n",
    "    history = model2.fit(x=array_x_train, y=array_y_train, batch_size=batches_size, \n",
    "                         epochs=nr_epochs, validation_data=(x_val, y_val), verbose=1,\n",
    "                          callbacks=callbacks_list,shuffle=True, initial_epoch=0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained model and evaluate with confusion matrix metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights('weights>40epochs/weights-improvement-47-0.88.hdf5')\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.001)\n",
    "model2.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model2.evaluate(array_x_test, array_y_test, batch_size=16)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_keras = model2.predict(array_x_test).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(array_y_test, y_pred_keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr,tpr): \n",
    "    plt.plot(fpr,tpr) \n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.show()    \n",
    "  \n",
    "plot_roc_curve(fpr_keras,tpr_keras) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "auc_score=roc_auc_score(array_y_test, y_pred_keras) \n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score , recall_score\n",
    " \n",
    "y_val_pred=model2.predict_classes(array_x_test)\n",
    " \n",
    "print(precision_score(array_y_test,y_val_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(array_y_test,y_val_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    " \n",
    "print(classification_report(array_y_test, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on movies fight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"thesis-data-augmentation-data\"\n",
    "file = \"leeftijd_top_10.xlsx\"\n",
    "file_X_MoviesFights = \"X_data_MoviesFights.npy\"\n",
    "\n",
    "file_Y_MoviesFights = \"Y_data_MoviesFights.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "obj_x_moviesfight = client.get_object(Bucket= bucket, Key=file_X_MoviesFights)\n",
    "obj_y_moviesfight = client.get_object(Bucket= bucket, Key=file_Y_MoviesFights)\n",
    "\n",
    "array_x_moviesfight = np.load(BytesIO(obj_x_moviesfight['Body'].read()))\n",
    "\n",
    "array_y_moviesfight = np.load(BytesIO(obj_y_moviesfight['Body'].read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained model and evaluate with confusion matrix metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights('weights>40epochs/weights-improvement-47-0.88.hdf5')\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.001)\n",
    "\n",
    "model2.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the movies fight data using `evaluate`\n",
    "print('\\n# Evaluate on movies fight data')\n",
    "results = model2.evaluate(array_x_moviesfight, array_y_moviesfight, batch_size=16)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_keras = model2.predict(array_x_moviesfight).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(array_y_moviesfight, y_pred_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr,tpr): \n",
    "    plt.plot(fpr,tpr) \n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.show()    \n",
    "  \n",
    "plot_roc_curve(fpr_keras,tpr_keras) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "auc_score=roc_auc_score(array_y_moviesfight, y_pred_keras)  \n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score , recall_score\n",
    " \n",
    "y_val_pred=model2.predict_classes(array_x_moviesfight)\n",
    " \n",
    "print(precision_score(array_y_moviesfight,y_val_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(array_y_moviesfight,y_val_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    " \n",
    "print(classification_report(array_y_moviesfight, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on crowding fight data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset is too big to load in one time, so it is loaded in parts and merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"thesis-data-augmentation-data\"\n",
    "file = \"leeftijd_top_10.xlsx\"\n",
    "file_X_CrowdFights_Part1 = \"X_data_CrowdFights_Part1.npy\"\n",
    "file_X_CrowdFights_Part2 = \"X_data_CrowdFights_Part2.npy\"\n",
    "file_X_CrowdFights_Part3 = \"X_data_CrowdFights_Part3.npy\"\n",
    "file_X_CrowdFights_Part4 = \"X_data_CrowdFights_Part4.npy\"\n",
    "\n",
    "file_Y_CrowdFights_Part1 = \"Y_data_CrowdFights_Part1.npy\"\n",
    "file_Y_CrowdFights_Part2 = \"Y_data_CrowdFights_Part2.npy\"\n",
    "file_Y_CrowdFights_Part3 = \"Y_data_CrowdFights_Part3.npy\"\n",
    "file_Y_CrowdFights_Part4 = \"Y_data_CrowdFights_Part4.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "obj_x_CrowdFights_Part1 = client.get_object(Bucket= bucket, Key=file_X_CrowdFights_Part1)\n",
    "obj_x_CrowdFights_Part2 = client.get_object(Bucket= bucket, Key=file_X_CrowdFights_Part2)\n",
    "obj_x_CrowdFights_Part3 = client.get_object(Bucket= bucket, Key=file_X_CrowdFights_Part3)\n",
    "obj_x_CrowdFights_Part4 = client.get_object(Bucket= bucket, Key=file_X_CrowdFights_Part4)\n",
    "\n",
    "obj_y_CrowdFights_Part1 = client.get_object(Bucket= bucket, Key=file_Y_CrowdFights_Part1)\n",
    "obj_y_CrowdFights_Part2 = client.get_object(Bucket= bucket, Key=file_Y_CrowdFights_Part2)\n",
    "obj_y_CrowdFights_Part3 = client.get_object(Bucket= bucket, Key=file_Y_CrowdFights_Part3)\n",
    "obj_y_CrowdFights_Part4 = client.get_object(Bucket= bucket, Key=file_Y_CrowdFights_Part4)\n",
    "\n",
    "array_x_CrowdFights_Part1 = np.load(BytesIO(obj_x_CrowdFights_Part1['Body'].read()))\n",
    "array_x_CrowdFights_Part2 = np.load(BytesIO(obj_x_CrowdFights_Part2['Body'].read()))\n",
    "array_x_CrowdFights_Part3 = np.load(BytesIO(obj_x_CrowdFights_Part3['Body'].read()))\n",
    "array_x_CrowdFights_Part4 = np.load(BytesIO(obj_x_CrowdFights_Part4['Body'].read()))\n",
    "\n",
    "array_y_CrowdFights_Part1 = np.load(BytesIO(obj_y_CrowdFights_Part1['Body'].read()))\n",
    "array_y_CrowdFights_Part2 = np.load(BytesIO(obj_y_CrowdFights_Part2['Body'].read()))\n",
    "array_y_CrowdFights_Part3 = np.load(BytesIO(obj_y_CrowdFights_Part3['Body'].read()))\n",
    "array_y_CrowdFights_Part4 = np.load(BytesIO(obj_y_CrowdFights_Part4['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xdata_concatenated_arrayCrowdFights = np.concatenate((array_x_CrowdFights_Part1,array_x_CrowdFights_Part2,array_x_CrowdFights_Part3,array_x_CrowdFights_Part4))\n",
    "print('Xdata CrowdFights concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ydata_concatenated_arrayCrowdFights = np.concatenate((array_y_CrowdFights_Part1,array_y_CrowdFights_Part2,array_y_CrowdFights_Part3,array_y_CrowdFights_Part4))\n",
    "print('Ydata CrowdFights concatenated succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained model and evaluate with confusion matrix metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model2.load_weights('weights>40epochs/weights-improvement-47-0.88.hdf5')\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.001)\n",
    "\n",
    "model2.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the crowd fight data using `evaluate`\n",
    "print('\\n# Evaluate on crowd fight data')\n",
    "results = model2.evaluate(xdata_concatenated_arrayCrowdFights, ydata_concatenated_arrayCrowdFights, batch_size=16)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_keras = model2.predict(xdata_concatenated_arrayCrowdFights).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(ydata_concatenated_arrayCrowdFights, y_pred_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr,tpr): \n",
    "    plt.plot(fpr,tpr) \n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.show()    \n",
    "  \n",
    "plot_roc_curve(fpr_keras,tpr_keras) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "auc_score=roc_auc_score(ydata_concatenated_arrayCrowdFights, y_pred_keras) \n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score , recall_score\n",
    " \n",
    "y_val_pred=model2.predict_classes(xdata_concatenated_arrayCrowdFights)\n",
    " \n",
    "print(precision_score(ydata_concatenated_arrayCrowdFights,y_val_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(ydata_concatenated_arrayCrowdFights,y_val_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    " \n",
    "print(classification_report(ydata_concatenated_arrayCrowdFights, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on hockey fight data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset is too big to load in one time, so it is loaded in parts and merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"thesis-data-augmentation-data\"\n",
    "file = \"leeftijd_top_10.xlsx\"\n",
    "file_X_HockeyFights_Part1 = \"X_data_HockeyFights_Part1.npy\"\n",
    "file_X_HockeyFights_Part2 = \"X_data_HockeyFights_Part2.npy\"\n",
    "file_X_HockeyFights_Part3 = \"X_data_HockeyFights_Part3.npy\"\n",
    "file_X_HockeyFights_Part4 = \"X_data_HockeyFights_Part4.npy\"\n",
    "file_X_HockeyFights_Part5 = \"X_data_HockeyFights_Part5.npy\"\n",
    "file_X_HockeyFights_Part6 = \"X_data_HockeyFights_Part6.npy\"\n",
    "file_X_HockeyFights_Part7 = \"X_data_HockeyFights_Part7.npy\"\n",
    "\n",
    "\n",
    "file_Y_HockeyFights_Part1 = \"Y_data_HockeyFights_Part1.npy\"\n",
    "file_Y_HockeyFights_Part2 = \"Y_data_HockeyFights_Part2.npy\"\n",
    "file_Y_HockeyFights_Part3 = \"Y_data_HockeyFights_Part3.npy\"\n",
    "file_Y_HockeyFights_Part4 = \"Y_data_HockeyFights_Part4.npy\"\n",
    "file_Y_HockeyFights_Part5 = \"Y_data_HockeyFights_Part5.npy\"\n",
    "file_Y_HockeyFights_Part6 = \"Y_data_HockeyFights_Part6.npy\"\n",
    "file_Y_HockeyFights_Part7 = \"Y_data_HockeyFights_Part7.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "obj_x_HockeyFights_Part1 = client.get_object(Bucket= bucket, Key=file_X_HockeyFights_Part1)\n",
    "obj_x_HockeyFights_Part2 = client.get_object(Bucket= bucket, Key=file_X_HockeyFights_Part2)\n",
    "obj_x_HockeyFights_Part3 = client.get_object(Bucket= bucket, Key=file_X_HockeyFights_Part3)\n",
    "obj_x_HockeyFights_Part4 = client.get_object(Bucket= bucket, Key=file_X_HockeyFights_Part4)\n",
    "obj_x_HockeyFights_Part5 = client.get_object(Bucket= bucket, Key=file_X_HockeyFights_Part5)\n",
    "obj_x_HockeyFights_Part6 = client.get_object(Bucket= bucket, Key=file_X_HockeyFights_Part6)\n",
    "obj_x_HockeyFights_Part7 = client.get_object(Bucket= bucket, Key=file_X_HockeyFights_Part7)\n",
    "\n",
    "obj_y_HockeyFights_Part1 = client.get_object(Bucket= bucket, Key=file_Y_HockeyFights_Part1)\n",
    "obj_y_HockeyFights_Part2 = client.get_object(Bucket= bucket, Key=file_Y_HockeyFights_Part2)\n",
    "obj_y_HockeyFights_Part3 = client.get_object(Bucket= bucket, Key=file_Y_HockeyFights_Part3)\n",
    "obj_y_HockeyFights_Part4 = client.get_object(Bucket= bucket, Key=file_Y_HockeyFights_Part4)\n",
    "obj_y_HockeyFights_Part5 = client.get_object(Bucket= bucket, Key=file_Y_HockeyFights_Part5)\n",
    "obj_y_HockeyFights_Part6 = client.get_object(Bucket= bucket, Key=file_Y_HockeyFights_Part6)\n",
    "obj_y_HockeyFights_Part7 = client.get_object(Bucket= bucket, Key=file_Y_HockeyFights_Part7)\n",
    "\n",
    "array_x_HockeyFights_Part1 = np.load(BytesIO(obj_x_HockeyFights_Part1['Body'].read()))\n",
    "array_x_HockeyFights_Part2 = np.load(BytesIO(obj_x_HockeyFights_Part2['Body'].read()))\n",
    "array_x_HockeyFights_Part3 = np.load(BytesIO(obj_x_HockeyFights_Part3['Body'].read()))\n",
    "array_x_HockeyFights_Part4 = np.load(BytesIO(obj_x_HockeyFights_Part4['Body'].read()))\n",
    "array_x_HockeyFights_Part5 = np.load(BytesIO(obj_x_HockeyFights_Part5['Body'].read()))\n",
    "array_x_HockeyFights_Part6 = np.load(BytesIO(obj_x_HockeyFights_Part6['Body'].read()))\n",
    "array_x_HockeyFights_Part7 = np.load(BytesIO(obj_x_HockeyFights_Part7['Body'].read()))\n",
    "\n",
    "array_y_HockeyFights_Part1 = np.load(BytesIO(obj_y_HockeyFights_Part1['Body'].read()))\n",
    "array_y_HockeyFights_Part2 = np.load(BytesIO(obj_y_HockeyFights_Part2['Body'].read()))\n",
    "array_y_HockeyFights_Part3 = np.load(BytesIO(obj_y_HockeyFights_Part3['Body'].read()))\n",
    "array_y_HockeyFights_Part4 = np.load(BytesIO(obj_y_HockeyFights_Part4['Body'].read()))\n",
    "array_y_HockeyFights_Part5 = np.load(BytesIO(obj_y_HockeyFights_Part5['Body'].read()))\n",
    "array_y_HockeyFights_Part6 = np.load(BytesIO(obj_y_HockeyFights_Part6['Body'].read()))\n",
    "array_y_HockeyFights_Part7 = np.load(BytesIO(obj_y_HockeyFights_Part7['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xdata HockeyFights concatenated succesfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xdata_concatenated_arrayHockeyFights = np.concatenate((array_x_HockeyFights_Part1,array_x_HockeyFights_Part2,array_x_HockeyFights_Part3,array_x_HockeyFights_Part4,array_x_HockeyFights_Part5,array_x_HockeyFights_Part6,array_x_HockeyFights_Part7))\n",
    "print('Xdata HockeyFights concatenated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ydata HockeyFights concatenated succesfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ydata_concatenated_arrayHockeyFights = np.concatenate((array_y_HockeyFights_Part1,array_y_HockeyFights_Part2,array_y_HockeyFights_Part3,array_y_HockeyFights_Part4,array_y_HockeyFights_Part5,array_y_HockeyFights_Part6,array_y_HockeyFights_Part7))\n",
    "print('Ydata HockeyFights concatenated succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained model and evaluate with confusion matrix metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model2.load_weights('weights>40epochs/weights-improvement-47-0.88.hdf5')\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "sgd = SGD(lr=0.001)\n",
    "\n",
    "model2.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on hockey fights data\n",
      "5018/5018 [==============================] - 331s 66ms/sample - loss: 1.4945 - acc: 0.7176\n",
      "test loss, test acc: [1.4945166171333524, 0.71761656]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the hockey fight data using `evaluate`\n",
    "print('\\n# Evaluate on hockey fights data')\n",
    "with tf.device('/gpu:0'):\n",
    "    results = model2.evaluate(xdata_concatenated_arrayHockeyFights, ydata_concatenated_arrayHockeyFights, batch_size=16)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_keras = model2.predict(xdata_concatenated_arrayHockeyFights).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(ydata_concatenated_arrayHockeyFights, y_pred_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc_curve(fpr,tpr): \n",
    "    plt.plot(fpr,tpr) \n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.show()    \n",
    "  \n",
    "plot_roc_curve(fpr_keras,tpr_keras) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8415015885623511"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "auc_score=roc_auc_score(ydata_concatenated_arrayHockeyFights, y_pred_keras) \n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65197867 0.87835052]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score , recall_score\n",
    " \n",
    "y_val_pred=model2.predict_classes(xdata_concatenated_arrayHockeyFights)\n",
    " \n",
    "print(precision_score(ydata_concatenated_arrayHockeyFights,y_val_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9292     0.50754567]\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(ydata_concatenated_arrayHockeyFights,y_val_pred,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.93      0.77      2500\n",
      "           1       0.88      0.51      0.64      2518\n",
      "\n",
      "    accuracy                           0.72      5018\n",
      "   macro avg       0.77      0.72      0.70      5018\n",
      "weighted avg       0.77      0.72      0.70      5018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    " \n",
    "print(classification_report(ydata_concatenated_arrayHockeyFights, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
